{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "V28"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "### **NEW**"
      ],
      "metadata": {
        "id": "AaOVlGFfJvPb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install tensorflow pandas numpy scikit-learn matplotlib opencv-python\n"
      ],
      "metadata": {
        "id": "esHk4WBOJube"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import json\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import cv2\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models, optimizers, callbacks\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import matplotlib.pyplot as plt\n"
      ],
      "metadata": {
        "id": "lznYkUN-Jr0K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Adjust the path to point to your specific dataset location\n",
        "LEFT_EYE_DIR = '/content/drive/My Drive/AllGalaxy/allgalaxy-webgazer/Data collection/Data/UnityEyes_Windows/left_eye/'\n",
        "RIGHT_EYE_DIR = '/content/drive/My Drive/AllGalaxy/allgalaxy-webgazer/Data collection/Data/UnityEyes_Windows/right_eye/'\n"
      ],
      "metadata": {
        "id": "UUqFQDbzw6Hm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)\n"
      ],
      "metadata": {
        "id": "PIInVpPUFh-7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "LEFT_EYE_DIR = '/content/drive/MyDrive/AllGalaxy/allgalaxy-webgazer/Data collection/Data/UnityEyes_Windows/UnityEyes_Windows/left_eye/'\n",
        "RIGHT_EYE_DIR = '/content/drive/MyDrive/AllGalaxy/allgalaxy-webgazer/Data collection/Data/UnityEyes_Windows/UnityEyes_Windows/right_eye/'\n"
      ],
      "metadata": {
        "id": "RvGdfahfFlEr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import random\n",
        "import json\n",
        "import cv2\n",
        "import numpy as np\n",
        "from google.colab import drive\n",
        "\n",
        "# Mount Google Drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Define Google Drive paths\n",
        "LEFT_EYE_DIR = '/content/drive/MyDrive/AllGalaxy/allgalaxy-webgazer/Data collection/Data/UnityEyes_Windows/UnityEyes_Windows/left_eye'\n",
        "RIGHT_EYE_DIR = '/content/drive/MyDrive/AllGalaxy/allgalaxy-webgazer/Data collection/Data/UnityEyes_Windows/UnityEyes_Windows/right_eye'\n",
        "\n",
        "# Initialize lists to store data\n",
        "images = []\n",
        "json_features = []\n",
        "labels = []\n",
        "\n",
        "# Function to parse JSON features\n",
        "def parse_json(json_path):\n",
        "    try:\n",
        "        with open(json_path, 'r') as f:\n",
        "            data = json.load(f)\n",
        "        features = []\n",
        "        head_pose = data.get('head_pose', \"(0,0,0)\").strip(\"()\").split(\",\")\n",
        "        head_pose = [float(angle) for angle in head_pose]\n",
        "        features.extend(head_pose)\n",
        "        eye_details = data.get('eye_details', {})\n",
        "        pupil_size = float(eye_details.get('pupil_size', 0.0))\n",
        "        iris_size = float(eye_details.get('iris_size', 0.0))\n",
        "        features.extend([pupil_size, iris_size])\n",
        "        return features\n",
        "    except (json.JSONDecodeError, FileNotFoundError) as e:\n",
        "        print(f\"Error loading JSON file {json_path}: {e}\")\n",
        "        return [0.0, 0.0, 0.0, 0.0, 0.0]\n",
        "\n",
        "# Function to process a single file (image and corresponding JSON)\n",
        "def process_file(directory, file):\n",
        "    img_path = os.path.join(directory, file)\n",
        "    json_path = os.path.join(directory, file.replace('.jpg', '.json'))\n",
        "\n",
        "    # Load and preprocess image\n",
        "    img = cv2.imread(img_path)\n",
        "    if img is None:\n",
        "        print(f\"Error loading image {img_path}\")\n",
        "        return None, None, None\n",
        "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "    img = cv2.resize(img, (224, 224))  # Resize to match model input size\n",
        "    img = img / 255.0  # Normalize to range [0, 1]\n",
        "\n",
        "    # Load and parse JSON features\n",
        "    features = parse_json(json_path)\n",
        "\n",
        "    # Placeholder label (replace with actual labels if available)\n",
        "    label = [0, 0]\n",
        "\n",
        "    return img, features, label\n",
        "\n",
        "# Function to randomly select and process 1000 images\n",
        "def process_random_images(directory, num_images=500):\n",
        "    try:\n",
        "        files = [file for file in os.listdir(directory) if file.endswith('.jpg')]\n",
        "        if len(files) > num_images:\n",
        "            selected_files = random.sample(files, num_images)  # Randomly select 1000 images\n",
        "        else:\n",
        "            selected_files = files\n",
        "\n",
        "        for file in selected_files:\n",
        "            img, features, label = process_file(directory, file)\n",
        "            if img is not None:\n",
        "                images.append(img)\n",
        "                json_features.append(features)\n",
        "                labels.append(label)\n",
        "    except OSError as e:\n",
        "        print(f\"Error processing files in directory {directory}: {e}\")\n",
        "\n",
        "# Process 1000 random images from each directory\n",
        "print(\"Processing 500 random images from left_eye directory\")\n",
        "process_random_images(LEFT_EYE_DIR, num_images=500)\n",
        "\n",
        "print(\"Processing 500 random images from right_eye directory\")\n",
        "process_random_images(RIGHT_EYE_DIR, num_images=500)\n",
        "\n",
        "# Convert lists to numpy arrays\n",
        "images = np.array(images)\n",
        "json_features = np.array(json_features)\n",
        "labels = np.array(labels)\n",
        "\n",
        "# Print shapes for confirmation\n",
        "print(f'Images shape: {images.shape}')\n",
        "print(f'JSON features shape: {json_features.shape}')\n",
        "print(f'Labels shape: {labels.shape}')\n"
      ],
      "metadata": {
        "id": "UQfu5ckWUrSk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Standardize JSON features\n",
        "scaler = StandardScaler()\n",
        "json_features = scaler.fit_transform(json_features)\n"
      ],
      "metadata": {
        "id": "L34Od5HpJ35a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Split into training and testing sets\n",
        "X_img_train, X_img_temp, X_json_train, X_json_temp, y_train, y_temp = train_test_split(\n",
        "    images, json_features, labels, test_size=0.3, random_state=42\n",
        ")\n",
        "\n",
        "# Further split temp into validation and testing\n",
        "X_img_val, X_img_test, X_json_val, X_json_test, y_val, y_test = train_test_split(\n",
        "    X_img_temp, X_json_temp, y_temp, test_size=0.5, random_state=42\n",
        ")\n",
        "\n",
        "print(f'Training images: {X_img_train.shape}')\n",
        "print(f'Validation images: {X_img_val.shape}')\n",
        "print(f'Testing images: {X_img_test.shape}')\n"
      ],
      "metadata": {
        "id": "DcVM0t4qJ4j9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Image input\n",
        "image_input = layers.Input(shape=(224, 224, 3), name='image_input')\n",
        "\n",
        "# Pre-trained CNN for image feature extraction\n",
        "base_model = tf.keras.applications.ResNet50(\n",
        "    include_top=False, weights='imagenet', input_tensor=image_input\n",
        ")\n",
        "base_model.trainable = False  # Freeze the base model\n",
        "\n",
        "# Add global pooling\n",
        "x = layers.GlobalAveragePooling2D()(base_model.output)\n",
        "x = layers.Dense(512, activation='relu')(x)\n",
        "x = layers.Dropout(0.5)(x)\n",
        "image_features = layers.Dense(256, activation='relu')(x)\n",
        "\n",
        "# JSON features input\n",
        "json_input = layers.Input(shape=(X_json_train.shape[1],), name='json_input')\n",
        "y = layers.Dense(128, activation='relu')(json_input)\n",
        "y = layers.Dropout(0.3)(y)\n",
        "json_features_dense = layers.Dense(64, activation='relu')(y)\n",
        "\n",
        "# Combine image and JSON features\n",
        "combined = layers.concatenate([image_features, json_features_dense])\n",
        "\n",
        "# Add fully connected layers\n",
        "z = layers.Dense(256, activation='relu')(combined)\n",
        "z = layers.Dropout(0.5)(z)\n",
        "z = layers.Dense(128, activation='relu')(z)\n",
        "\n",
        "# Output layer\n",
        "# For regression (e.g., gaze x and y coordinates)\n",
        "output = layers.Dense(2, activation='linear', name='output')(z)\n",
        "\n",
        "# Define the model\n",
        "model = models.Model(inputs=[image_input, json_input], outputs=output)\n",
        "\n",
        "# Compile the model\n",
        "model.compile(\n",
        "    optimizer=optimizers.Adam(learning_rate=1e-4),\n",
        "    loss='mean_squared_error',\n",
        "    metrics=['mae']\n",
        ")\n",
        "\n",
        "model.summary()\n"
      ],
      "metadata": {
        "id": "5E_TNfUNJ8yF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "# Define ImageDataGenerator for augmentation\n",
        "datagen = ImageDataGenerator(\n",
        "    rotation_range=10,\n",
        "    zoom_range=0.1,\n",
        "    horizontal_flip=True,\n",
        "    brightness_range=[0.8,1.2]\n",
        ")\n",
        "\n",
        "# Example of applying augmentation\n",
        "# Note: When using multiple inputs, custom generators might be needed\n"
      ],
      "metadata": {
        "id": "JabJ_UMPJ9rb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define callbacks\n",
        "checkpoint = callbacks.ModelCheckpoint(\n",
        "    'best_model.h5', monitor='val_loss', save_best_only=True, mode='min'\n",
        ")\n",
        "early_stop = callbacks.EarlyStopping(\n",
        "    monitor='val_loss', patience=10, restore_best_weights=True\n",
        ")\n",
        "\n",
        "# Train the model\n",
        "history = model.fit(\n",
        "    {'image_input': X_img_train, 'json_input': X_json_train},\n",
        "    y_train,\n",
        "    epochs=100,\n",
        "    batch_size=32,\n",
        "    validation_data=(\n",
        "        {'image_input': X_img_val, 'json_input': X_json_val},\n",
        "        y_val\n",
        "    ),\n",
        "    callbacks=[checkpoint, early_stop]\n",
        ")\n"
      ],
      "metadata": {
        "id": "eKvONHzyKAPW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the best model\n",
        "model.load_weights('best_model.h5')\n",
        "\n",
        "# Evaluate on test data\n",
        "test_loss, test_mae = model.evaluate(\n",
        "    {'image_input': X_img_test, 'json_input': X_json_test},\n",
        "    y_test\n",
        ")\n",
        "\n",
        "print(f'Test Loss: {test_loss}')\n",
        "print(f'Test MAE: {test_mae}')\n"
      ],
      "metadata": {
        "id": "5rVQFV0VKEyS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot training & validation loss values\n",
        "plt.figure(figsize=(12, 4))\n",
        "\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(history.history['loss'], label='Train Loss')\n",
        "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
        "plt.title('Model Loss')\n",
        "plt.ylabel('MSE Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(loc='upper right')\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(history.history['mae'], label='Train MAE')\n",
        "plt.plot(history.history['val_mae'], label='Validation MAE')\n",
        "plt.title('Model MAE')\n",
        "plt.ylabel('Mean Absolute Error')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(loc='upper right')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "CvIHSmoaKFfu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import random\n",
        "import cv2\n",
        "import numpy as np\n",
        "import json\n",
        "\n",
        "# Paths to your left and right eye data in Google Drive\n",
        "LEFT_EYE_DIR = '/content/drive/MyDrive/AllGalaxy/allgalaxy-webgazer/Data collection/Data/UnityEyes_Windows/UnityEyes_Windows/left_eye'\n",
        "RIGHT_EYE_DIR = '/content/drive/MyDrive/AllGalaxy/allgalaxy-webgazer/Data collection/Data/UnityEyes_Windows/UnityEyes_Windows/right_eye'\n",
        "\n",
        "# Function to parse JSON features\n",
        "def parse_json(json_path):\n",
        "    try:\n",
        "        with open(json_path, 'r') as f:\n",
        "            data = json.load(f)\n",
        "        features = []\n",
        "        head_pose = data.get('head_pose', \"(0,0,0)\").strip(\"()\").split(\",\")\n",
        "        head_pose = [float(angle) for angle in head_pose]\n",
        "        features.extend(head_pose)\n",
        "        eye_details = data.get('eye_details', {})\n",
        "        pupil_size = float(eye_details.get('pupil_size', 0.0))\n",
        "        iris_size = float(eye_details.get('iris_size', 0.0))\n",
        "        features.extend([pupil_size, iris_size])\n",
        "        return features\n",
        "    except (json.JSONDecodeError, FileNotFoundError) as e:\n",
        "        print(f\"Error loading JSON file {json_path}: {e}\")\n",
        "        return None\n",
        "\n",
        "# Function to load and preprocess an image\n",
        "def load_and_preprocess_image(image_path):\n",
        "    image = cv2.imread(image_path)\n",
        "\n",
        "    if image is None:\n",
        "        print(f\"Error loading image at {image_path}\")\n",
        "        return None\n",
        "\n",
        "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "    image = cv2.resize(image, (224, 224))  # Resize to match model input size\n",
        "    image = image / 255.0  # Normalize to range [0, 1]\n",
        "    image = np.expand_dims(image, axis=0)  # Expand dims to match input shape\n",
        "\n",
        "    return image\n",
        "\n",
        "# Select a random file from the left_eye directory\n",
        "def get_random_image_and_json(directory):\n",
        "    try:\n",
        "        # Get all .jpg files in the directory\n",
        "        files = [f for f in os.listdir(directory) if f.endswith('.jpg')]\n",
        "        if not files:\n",
        "            print(f\"No images found in directory: {directory}\")\n",
        "            return None, None\n",
        "\n",
        "        # Select a random file\n",
        "        random_file = random.choice(files)\n",
        "\n",
        "        # Get the corresponding JSON path\n",
        "        image_path = os.path.join(directory, random_file)\n",
        "        json_path = os.path.join(directory, random_file.replace('.jpg', '.json'))\n",
        "\n",
        "        # Check if the corresponding JSON file exists\n",
        "        if not os.path.exists(json_path):\n",
        "            print(f\"Error: Corresponding JSON file {json_path} not found.\")\n",
        "            return None, None\n",
        "\n",
        "        return image_path, json_path\n",
        "    except Exception as e:\n",
        "        print(f\"Error in get_random_image_and_json: {e}\")\n",
        "        return None, None\n",
        "\n",
        "# Load a random image and JSON from the left_eye directory for prediction\n",
        "image_path, json_path = get_random_image_and_json(LEFT_EYE_DIR)\n",
        "\n",
        "if image_path and json_path:\n",
        "    # Load and preprocess the image\n",
        "    new_image = load_and_preprocess_image(image_path)\n",
        "\n",
        "    # Load and preprocess the JSON data\n",
        "    new_json = parse_json(json_path)\n",
        "\n",
        "    if new_image is not None and new_json is not None:\n",
        "        # Assuming you have a trained scaler (replace 'scaler' with your actual scaler)\n",
        "        new_json = scaler.transform([new_json])\n",
        "\n",
        "        # Predict gaze coordinates (assuming your model expects 'image_input' and 'json_input' as inputs)\n",
        "        prediction = model.predict({'image_input': new_image, 'json_input': new_json})\n",
        "        print(f'Predicted Gaze Coordinates: {prediction[0]}')\n",
        "    else:\n",
        "        print(\"Error: Could not load image or JSON data.\")\n",
        "else:\n",
        "    print(\"Error: Could not retrieve a random image and its corresponding JSON.\")\n"
      ],
      "metadata": {
        "id": "u5zZQpLrKIom"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Install necessary libraries (if not pre-installed)\n",
        "!pip install opencv-python-headless\n",
        "import cv2\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from google.colab.patches import cv2_imshow  # Import cv2_imshow for displaying in Colab\n",
        "\n",
        "# Load your pre-trained model (upload the model file first to Colab)\n",
        "model = tf.keras.models.load_model('/content/best_model.h5')\n",
        "\n",
        "# Function to preprocess video frame before feeding to the model\n",
        "def preprocess_frame(frame):\n",
        "    gray_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)  # Convert to grayscale\n",
        "    resized_frame = cv2.resize(gray_frame, (64, 64))  # Resize to the model's input size\n",
        "    normalized_frame = resized_frame / 255.0  # Normalize the pixel values\n",
        "    return np.expand_dims(np.expand_dims(normalized_frame, axis=-1), axis=0)\n",
        "\n",
        "# Function to process video stream and track eye movements\n",
        "def process_and_track_eye(video_source=0, save_to_file='eye_tracking_data.csv'):\n",
        "    # Initialize video capture (video_source=0 for webcam)\n",
        "    cap = cv2.VideoCapture(video_source)\n",
        "\n",
        "    # Open a CSV file to store the tracking data\n",
        "    with open(save_to_file, 'w') as f:\n",
        "        f.write('Frame, Eye_X, Eye_Y\\n')  # CSV header\n",
        "\n",
        "        while True:\n",
        "            ret, frame = cap.read()  # Read each frame\n",
        "            if not ret:\n",
        "                break\n",
        "\n",
        "            # Preprocess the frame for the model\n",
        "            processed_frame = preprocess_frame(frame)\n",
        "\n",
        "            # Predict eye position using the model (assumed output is (x, y) coordinates)\n",
        "            eye_position = model.predict(processed_frame)  # Model predicts the eye's x, y position\n",
        "\n",
        "            # Convert normalized eye coordinates to actual frame coordinates\n",
        "            eye_x = int(eye_position[0][0] * frame.shape[1])\n",
        "            eye_y = int(eye_position[0][1] * frame.shape[0])\n",
        "\n",
        "            # Draw a green dot at the predicted eye position\n",
        "            cv2.circle(frame, (eye_x, eye_y), 5, (0, 255, 0), -1)\n",
        "\n",
        "            # Display the frame with the green dot\n",
        "            cv2_imshow(frame)  # Use cv2_imshow in Colab\n",
        "\n",
        "            # Save the eye-tracking data (frame number, eye_x, eye_y) to the CSV file\n",
        "            f.write(f'{int(cap.get(cv2.CAP_PROP_POS_FRAMES))}, {eye_x}, {eye_y}\\n')\n",
        "\n",
        "            # Stop the loop manually by breaking out (optional)\n",
        "            if cv2.waitKey(1) & 0xFF == ord('q'):\n",
        "                break\n",
        "\n",
        "    # Release the video capture\n",
        "    cap.release()\n",
        "\n",
        "# Run the eye tracking on webcam stream\n",
        "process_and_track_eye()\n"
      ],
      "metadata": {
        "id": "-4hHUB4Byavg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import display, Javascript\n",
        "from google.colab.output import eval_js\n",
        "import base64\n",
        "import cv2\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import time\n",
        "\n",
        "# JavaScript to handle the video stream and snapshot\n",
        "def video_stream():\n",
        "    js = Javascript('''\n",
        "    var video;\n",
        "    var div = null;\n",
        "    var stream;\n",
        "\n",
        "    async function streamVideo() {\n",
        "        div = document.createElement('div');\n",
        "        document.body.appendChild(div);\n",
        "        div.style.textAlign = 'center';\n",
        "\n",
        "        video = document.createElement('video');\n",
        "        video.style.display = 'block';\n",
        "        div.appendChild(video);\n",
        "\n",
        "        stream = await navigator.mediaDevices.getUserMedia({video: true});\n",
        "        video.srcObject = stream;\n",
        "        await video.play();\n",
        "\n",
        "        window.imgElement = document.createElement('img');\n",
        "        div.appendChild(window.imgElement);\n",
        "\n",
        "        window.captureCanvas = document.createElement('canvas');\n",
        "        captureCanvas.width = 224;\n",
        "        captureCanvas.height = 224;\n",
        "        captureCanvas.style.display = 'block';\n",
        "        div.appendChild(captureCanvas);\n",
        "\n",
        "        window.labelElement = document.createElement('div');\n",
        "        labelElement.innerText = 'Model output will appear here';\n",
        "        div.appendChild(labelElement);\n",
        "\n",
        "        var shutdown = false;\n",
        "        var pendingResolve = null;\n",
        "\n",
        "        function removeDom() {\n",
        "            stream.getVideoTracks()[0].stop();\n",
        "            if (div !== null) {\n",
        "                div.remove();\n",
        "                div = null;\n",
        "                video = null;\n",
        "                captureCanvas = null;\n",
        "                imgElement = null;\n",
        "                labelElement = null;\n",
        "            }\n",
        "        }\n",
        "\n",
        "        function onAnimationFrame() {\n",
        "            if (!shutdown) {\n",
        "                captureCanvas.getContext('2d').drawImage(video, 0, 0, 224, 224);\n",
        "                requestAnimationFrame(onAnimationFrame);\n",
        "            }\n",
        "        }\n",
        "\n",
        "        requestAnimationFrame(onAnimationFrame);\n",
        "\n",
        "        // Define takeSnapshot as a method of window\n",
        "        window.takeSnapshot = async function() {\n",
        "            return captureCanvas.toDataURL('image/jpeg', 0.8);\n",
        "        };\n",
        "\n",
        "        await new Promise((resolve) => {\n",
        "            pendingResolve = resolve;\n",
        "        });\n",
        "        shutdown = true;\n",
        "        removeDom();\n",
        "    }\n",
        "\n",
        "    streamVideo();\n",
        "    ''')\n",
        "    display(js)\n",
        "\n",
        "def get_frame():\n",
        "    data = eval_js('takeSnapshot()')\n",
        "    binary = base64.b64decode(data.split(',')[1])\n",
        "    image = np.frombuffer(binary, dtype=np.uint8)\n",
        "    image = cv2.imdecode(image, flags=cv2.IMREAD_COLOR)\n",
        "    return image\n",
        "\n",
        "# Load your pre-trained model\n",
        "model = tf.keras.models.load_model('/content/best_model.h5')\n",
        "\n",
        "# Start streaming video from webcam\n",
        "video_stream()\n",
        "\n",
        "# Wait a bit to let JavaScript code initialize\n",
        "time.sleep(2)\n",
        "\n",
        "# Perform a continuous loop to process the camera frames\n",
        "try:\n",
        "    while True:\n",
        "        frame = get_frame()\n",
        "        # Assuming model takes 224x224 RGB images\n",
        "        input_frame = cv2.resize(frame, (224, 224))\n",
        "        input_frame = np.expand_dims(input_frame, axis=0)\n",
        "\n",
        "        # Generate JSON input (dummy for example purposes)\n",
        "        json_input = np.zeros((1, 5))\n",
        "\n",
        "        # Combine inputs and predict\n",
        "        prediction = model.predict([input_frame, json_input])\n",
        "\n",
        "        # Assume model predicts x, y coordinates scaled [0, 1]\n",
        "        pred_x = int(prediction[0][0] * frame.shape[1])\n",
        "        pred_y = int(prediction[0][1] * frame.shape[0])\n",
        "\n",
        "        # Draw prediction on frame\n",
        "        cv2.circle(frame, (pred_x, pred_y), 10, (0, 255, 0), -1)\n",
        "\n",
        "        # Convert frame to JPEG format\n",
        "        _, jpeg_image = cv2.imencode('.jpg', frame)\n",
        "        data_url_image = 'data:image/jpeg;base64,' + base64.b64encode(jpeg_image).decode('utf-8')\n",
        "        eval_js(f'imgElement.src=\"{data_url_image}\"; labelElement.innerText=\"Model predicted coordinates: ({pred_x}, {pred_y})\";')\n",
        "\n",
        "        # Add a short delay to control frame rate\n",
        "        time.sleep(0.1)\n",
        "\n",
        "except KeyboardInterrupt:\n",
        "    pass\n"
      ],
      "metadata": {
        "id": "WGBYiZA90h_o"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}