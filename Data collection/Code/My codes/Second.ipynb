{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "V28"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "### **NEW**"
      ],
      "metadata": {
        "id": "AaOVlGFfJvPb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install tensorflow pandas numpy scikit-learn matplotlib opencv-python\n"
      ],
      "metadata": {
        "id": "esHk4WBOJube"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import json\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import cv2\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models, optimizers, callbacks\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import matplotlib.pyplot as plt\n",
        "from concurrent.futures import ThreadPoolExecutor\n",
        "import random\n",
        "import time\n",
        "\n",
        "# Mount Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Adjust the path to point to your specific dataset location\n",
        "LEFT_EYE_DIR = '/content/drive/MyDrive/AllGalaxy/allgalaxy-webgazer/Data collection/Data/UnityEyes_Windows/UnityEyes_Windows/left_eye'\n",
        "RIGHT_EYE_DIR = '/content/drive/MyDrive/AllGalaxy/allgalaxy-webgazer/Data collection/Data/UnityEyes_Windows/UnityEyes_Windows/right_eye'\n",
        "\n",
        "# Initialize lists to store data\n",
        "images = []\n",
        "json_features = []\n",
        "labels = []\n",
        "\n",
        "# Function to parse JSON features\n",
        "def parse_json(json_path):\n",
        "    try:\n",
        "        with open(json_path, 'r') as f:\n",
        "            data = json.load(f)\n",
        "        features = []\n",
        "\n",
        "        # Extract head pose and eye details\n",
        "        head_pose = [float(angle) for angle in data.get('head_pose', \"(0,0,0)\").strip(\"()\").split(\",\")]\n",
        "        eye_details = data.get('eye_details', {})\n",
        "        pupil_size = float(eye_details.get('pupil_size', 0.0))\n",
        "        iris_size = float(eye_details.get('iris_size', 0.0))\n",
        "\n",
        "        # Combine all features into a single list\n",
        "        features.extend(head_pose)\n",
        "        features.extend([pupil_size, iris_size])\n",
        "        return features\n",
        "    except (json.JSONDecodeError, FileNotFoundError) as e:\n",
        "        print(f\"Error loading JSON file {json_path}: {e}\")\n",
        "        return [0.0, 0.0, 0.0, 0.0, 0.0]\n",
        "\n",
        "# Function to process a single image file and corresponding JSON file\n",
        "def process_file(img_path, json_path):\n",
        "    try:\n",
        "        # Load and preprocess the image\n",
        "        img = cv2.imread(img_path)\n",
        "        if img is None:\n",
        "            print(f\"Error loading image {img_path}\")\n",
        "            return None, None, None\n",
        "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "        img = cv2.resize(img, (224, 224)) / 255.0  # Normalize to [0, 1]\n",
        "\n",
        "        # Load and parse JSON features\n",
        "        features = parse_json(json_path)\n",
        "\n",
        "        # Placeholder label (replace with actual labels if available)\n",
        "        label = [0, 0]\n",
        "\n",
        "        return img, features, label\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing file {img_path}: {e}\")\n",
        "        return None, None, None\n",
        "\n",
        "# Function to process random samples in batches to avoid memory overload and I/O issues\n",
        "def process_in_batches(directory, batch_size=100, num_samples=1000):\n",
        "    try:\n",
        "        # Get all JPG files and randomly sample them\n",
        "        files = [file for file in os.listdir(directory) if file.endswith('.jpg')]\n",
        "        total_files = len(files)\n",
        "\n",
        "        # Randomly sample num_samples files\n",
        "        sample_files = random.sample(files, min(num_samples, total_files))\n",
        "\n",
        "        # Process files in batches\n",
        "        for i in range(0, len(sample_files), batch_size):\n",
        "            batch_files = sample_files[i:i + batch_size]\n",
        "            print(f\"Processing batch {i // batch_size + 1} from {directory}...\")\n",
        "\n",
        "            for file in batch_files:\n",
        "                img_path = os.path.join(directory, file)\n",
        "                json_path = os.path.join(directory, file.replace('.jpg', '.json'))\n",
        "\n",
        "                img, features, label = process_file(img_path, json_path)\n",
        "                if img is not None:\n",
        "                    images.append(img)\n",
        "                    json_features.append(features)\n",
        "                    labels.append(label)\n",
        "\n",
        "            # Pause for a short time to reduce pressure on file system\n",
        "            time.sleep(1)\n",
        "\n",
        "    except OSError as e:\n",
        "        print(f\"Error processing files in directory {directory}: {e}\")\n",
        "\n",
        "# Process 1000 random files from each directory in batches of 100\n",
        "print(\"Processing samples from left_eye directory\")\n",
        "process_in_batches(LEFT_EYE_DIR, batch_size=100, num_samples=1000)\n",
        "\n",
        "print(\"Processing samples from right_eye directory\")\n",
        "process_in_batches(RIGHT_EYE_DIR, batch_size=100, num_samples=1000)\n",
        "\n",
        "# Convert lists to numpy arrays\n",
        "images = np.array(images)\n",
        "json_features = np.array(json_features)\n",
        "labels = np.array(labels)\n",
        "\n",
        "# Print shapes for confirmation\n",
        "print(f'Images shape: {images.shape}')\n",
        "print(f'JSON features shape: {json_features.shape}')\n",
        "print(f'Labels shape: {labels.shape}')\n",
        "\n",
        "# Example of how to use Matplotlib for visualization\n",
        "def visualize_data():\n",
        "    plt.figure(figsize=(10, 10))\n",
        "    for i in range(9):\n",
        "        plt.subplot(3, 3, i+1)\n",
        "        plt.imshow(images[i])\n",
        "        plt.title(f\"Label: {labels[i]}\")\n",
        "        plt.axis('off')\n",
        "    plt.show()\n",
        "\n",
        "# Call the visualization function if needed\n",
        "visualize_data()\n"
      ],
      "metadata": {
        "id": "kyNfWdfzJzcJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Standardize JSON features\n",
        "scaler = StandardScaler()\n",
        "json_features = scaler.fit_transform(json_features)\n",
        "# Split into training and testing sets\n",
        "X_img_train, X_img_temp, X_json_train, X_json_temp, y_train, y_temp = train_test_split(\n",
        "    images, json_features, labels, test_size=0.3, random_state=42\n",
        ")\n",
        "\n",
        "# Further split temp into validation and testing\n",
        "X_img_val, X_img_test, X_json_val, X_json_test, y_val, y_test = train_test_split(\n",
        "    X_img_temp, X_json_temp, y_temp, test_size=0.5, random_state=42\n",
        ")\n",
        "\n",
        "print(f'Training images: {X_img_train.shape}')\n",
        "print(f'Validation images: {X_img_val.shape}')\n",
        "print(f'Testing images: {X_img_test.shape}')\n",
        "# Image input\n",
        "image_input = layers.Input(shape=(224, 224, 3), name='image_input')\n",
        "\n",
        "# Pre-trained CNN for image feature extraction\n",
        "base_model = tf.keras.applications.ResNet50(\n",
        "    include_top=False, weights='imagenet', input_tensor=image_input\n",
        ")\n",
        "base_model.trainable = False  # Freeze the base model\n",
        "\n",
        "# Add global pooling\n",
        "x = layers.GlobalAveragePooling2D()(base_model.output)\n",
        "x = layers.Dense(512, activation='relu')(x)\n",
        "x = layers.Dropout(0.5)(x)\n",
        "image_features = layers.Dense(256, activation='relu')(x)\n",
        "\n",
        "# JSON features input\n",
        "json_input = layers.Input(shape=(X_json_train.shape[1],), name='json_input')\n",
        "y = layers.Dense(128, activation='relu')(json_input)\n",
        "y = layers.Dropout(0.3)(y)\n",
        "json_features_dense = layers.Dense(64, activation='relu')(y)\n",
        "\n",
        "# Combine image and JSON features\n",
        "combined = layers.concatenate([image_features, json_features_dense])\n",
        "\n",
        "# Add fully connected layers\n",
        "z = layers.Dense(256, activation='relu')(combined)\n",
        "z = layers.Dropout(0.5)(z)\n",
        "z = layers.Dense(128, activation='relu')(z)\n",
        "\n",
        "# Output layer\n",
        "# For regression (e.g., gaze x and y coordinates)\n",
        "output = layers.Dense(2, activation='linear', name='output')(z)\n",
        "\n",
        "# Define the model\n",
        "model = models.Model(inputs=[image_input, json_input], outputs=output)\n",
        "\n",
        "# Compile the model\n",
        "model.compile(\n",
        "    optimizer=optimizers.Adam(learning_rate=1e-4),\n",
        "    loss='mean_squared_error',\n",
        "    metrics=['mae']\n",
        ")\n",
        "\n",
        "model.summary()\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "# Define ImageDataGenerator for augmentation\n",
        "datagen = ImageDataGenerator(\n",
        "    rotation_range=10,\n",
        "    zoom_range=0.1,\n",
        "    horizontal_flip=True,\n",
        "    brightness_range=[0.8,1.2]\n",
        ")\n",
        "\n",
        "# Example of applying augmentation\n",
        "# Note: When using multiple inputs, custom generators might be needed"
      ],
      "metadata": {
        "id": "L34Od5HpJ35a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define callbacks\n",
        "checkpoint = callbacks.ModelCheckpoint(\n",
        "    'second.h5', monitor='val_loss', save_best_only=True, mode='min'\n",
        ")\n",
        "early_stop = callbacks.EarlyStopping(\n",
        "    monitor='val_loss', patience=10, restore_best_weights=True\n",
        ")\n",
        "\n",
        "# Train the model\n",
        "history = model.fit(\n",
        "    {'image_input': X_img_train, 'json_input': X_json_train},\n",
        "    y_train,\n",
        "    epochs=10,\n",
        "    batch_size=32,\n",
        "    validation_data=(\n",
        "        {'image_input': X_img_val, 'json_input': X_json_val},\n",
        "        y_val\n",
        "    ),\n",
        "    callbacks=[checkpoint, early_stop]\n",
        ")\n",
        "# Load the best model\n",
        "model.load_weights('second.h5')\n",
        "\n",
        "# Evaluate on test data\n",
        "test_loss, test_mae = model.evaluate(\n",
        "    {'image_input': X_img_test, 'json_input': X_json_test},\n",
        "    y_test\n",
        ")\n",
        "\n",
        "print(f'Test Loss: {test_loss}')\n",
        "print(f'Test MAE: {test_mae}')"
      ],
      "metadata": {
        "id": "eKvONHzyKAPW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot training & validation loss values\n",
        "plt.figure(figsize=(12, 4))\n",
        "\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(history.history['loss'], label='Train Loss')\n",
        "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
        "plt.title('Model Loss')\n",
        "plt.ylabel('MSE Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(loc='upper right')\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(history.history['mae'], label='Train MAE')\n",
        "plt.plot(history.history['val_mae'], label='Validation MAE')\n",
        "plt.title('Model MAE')\n",
        "plt.ylabel('Mean Absolute Error')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(loc='upper right')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "CvIHSmoaKFfu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import random\n",
        "import cv2\n",
        "import numpy as np\n",
        "import json\n",
        "\n",
        "# Paths to your left and right eye data in Google Drive\n",
        "LEFT_EYE_DIR = '/content/drive/MyDrive/AllGalaxy/allgalaxy-webgazer/Data collection/Data/UnityEyes_Windows/UnityEyes_Windows/left_eye'\n",
        "RIGHT_EYE_DIR = '/content/drive/MyDrive/AllGalaxy/allgalaxy-webgazer/Data collection/Data/UnityEyes_Windows/UnityEyes_Windows/right_eye'\n",
        "\n",
        "# Function to parse JSON features\n",
        "def parse_json(json_path):\n",
        "    try:\n",
        "        with open(json_path, 'r') as f:\n",
        "            data = json.load(f)\n",
        "        features = []\n",
        "        head_pose = data.get('head_pose', \"(0,0,0)\").strip(\"()\").split(\",\")\n",
        "        head_pose = [float(angle) for angle in head_pose]\n",
        "        features.extend(head_pose)\n",
        "        eye_details = data.get('eye_details', {})\n",
        "        pupil_size = float(eye_details.get('pupil_size', 0.0))\n",
        "        iris_size = float(eye_details.get('iris_size', 0.0))\n",
        "        features.extend([pupil_size, iris_size])\n",
        "        return features\n",
        "    except (json.JSONDecodeError, FileNotFoundError) as e:\n",
        "        print(f\"Error loading JSON file {json_path}: {e}\")\n",
        "        return None\n",
        "\n",
        "# Function to load and preprocess an image\n",
        "def load_and_preprocess_image(image_path):\n",
        "    image = cv2.imread(image_path)\n",
        "\n",
        "    if image is None:\n",
        "        print(f\"Error loading image at {image_path}\")\n",
        "        return None\n",
        "\n",
        "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "    image = cv2.resize(image, (224, 224))  # Resize to match model input size\n",
        "    image = image / 255.0  # Normalize to range [0, 1]\n",
        "    image = np.expand_dims(image, axis=0)  # Expand dims to match input shape\n",
        "\n",
        "    return image\n",
        "\n",
        "# Select a random file from the left_eye directory\n",
        "def get_random_image_and_json(directory):\n",
        "    try:\n",
        "        # Get all .jpg files in the directory\n",
        "        files = [f for f in os.listdir(directory) if f.endswith('.jpg')]\n",
        "        if not files:\n",
        "            print(f\"No images found in directory: {directory}\")\n",
        "            return None, None\n",
        "\n",
        "        # Select a random file\n",
        "        random_file = random.choice(files)\n",
        "\n",
        "        # Get the corresponding JSON path\n",
        "        image_path = os.path.join(directory, random_file)\n",
        "        json_path = os.path.join(directory, random_file.replace('.jpg', '.json'))\n",
        "\n",
        "        # Check if the corresponding JSON file exists\n",
        "        if not os.path.exists(json_path):\n",
        "            print(f\"Error: Corresponding JSON file {json_path} not found.\")\n",
        "            return None, None\n",
        "\n",
        "        return image_path, json_path\n",
        "    except Exception as e:\n",
        "        print(f\"Error in get_random_image_and_json: {e}\")\n",
        "        return None, None\n",
        "\n",
        "# Load a random image and JSON from the left_eye directory for prediction\n",
        "image_path, json_path = get_random_image_and_json(LEFT_EYE_DIR)\n",
        "\n",
        "if image_path and json_path:\n",
        "    # Load and preprocess the image\n",
        "    new_image = load_and_preprocess_image(image_path)\n",
        "\n",
        "    # Load and preprocess the JSON data\n",
        "    new_json = parse_json(json_path)\n",
        "\n",
        "    if new_image is not None and new_json is not None:\n",
        "        # Assuming you have a trained scaler (replace 'scaler' with your actual scaler)\n",
        "        new_json = scaler.transform([new_json])\n",
        "\n",
        "        # Predict gaze coordinates (assuming your model expects 'image_input' and 'json_input' as inputs)\n",
        "        prediction = model.predict({'image_input': new_image, 'json_input': new_json})\n",
        "        print(f'Predicted Gaze Coordinates: {prediction[0]}')\n",
        "    else:\n",
        "        print(\"Error: Could not load image or JSON data.\")\n",
        "else:\n",
        "    print(\"Error: Could not retrieve a random image and its corresponding JSON.\")\n"
      ],
      "metadata": {
        "id": "u5zZQpLrKIom"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import display, Javascript\n",
        "from google.colab.output import eval_js\n",
        "import base64\n",
        "import cv2\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import time\n",
        "import math\n",
        "\n",
        "# JavaScript to handle the video stream and snapshot\n",
        "def video_stream():\n",
        "    js = Javascript('''\n",
        "    var video;\n",
        "    var div = null;\n",
        "    var stream;\n",
        "\n",
        "    async function streamVideo() {\n",
        "        div = document.createElement('div');\n",
        "        document.body.appendChild(div);\n",
        "        div.style.textAlign = 'center';\n",
        "\n",
        "        video = document.createElement('video');\n",
        "        video.style.display = 'block';\n",
        "        div.appendChild(video);\n",
        "\n",
        "        stream = await navigator.mediaDevices.getUserMedia({video: true});\n",
        "        video.srcObject = stream;\n",
        "        await video.play();\n",
        "\n",
        "        window.imgElement = document.createElement('img');\n",
        "        div.appendChild(window.imgElement);\n",
        "\n",
        "        window.captureCanvas = document.createElement('canvas');\n",
        "        captureCanvas.width = 224;\n",
        "        captureCanvas.height = 224;\n",
        "        captureCanvas.style.display = 'block';\n",
        "        div.appendChild(captureCanvas);\n",
        "\n",
        "        window.labelElement = document.createElement('div');\n",
        "        labelElement.innerText = 'Model output will appear here';\n",
        "        div.appendChild(labelElement);\n",
        "\n",
        "        var shutdown = false;\n",
        "        var pendingResolve = null;\n",
        "\n",
        "        function removeDom() {\n",
        "            stream.getVideoTracks()[0].stop();\n",
        "            if (div !== null) {\n",
        "                div.remove();\n",
        "                div = null;\n",
        "                video = null;\n",
        "                captureCanvas = null;\n",
        "                imgElement = null;\n",
        "                labelElement = null;\n",
        "            }\n",
        "        }\n",
        "\n",
        "        function onAnimationFrame() {\n",
        "            if (!shutdown) {\n",
        "                captureCanvas.getContext('2d').drawImage(video, 0, 0, 224, 224);\n",
        "                requestAnimationFrame(onAnimationFrame);\n",
        "            }\n",
        "        }\n",
        "\n",
        "        requestAnimationFrame(onAnimationFrame);\n",
        "\n",
        "        // Define takeSnapshot as a method of window\n",
        "        window.takeSnapshot = async function() {\n",
        "            return captureCanvas.toDataURL('image/jpeg', 0.8);\n",
        "        };\n",
        "\n",
        "        await new Promise((resolve) => {\n",
        "            pendingResolve = resolve;\n",
        "        });\n",
        "        shutdown = true;\n",
        "        removeDom();\n",
        "    }\n",
        "\n",
        "    streamVideo();\n",
        "    ''')\n",
        "    display(js)\n",
        "\n",
        "def get_frame():\n",
        "    data = eval_js('takeSnapshot()')\n",
        "    binary = base64.b64decode(data.split(',')[1])\n",
        "    image = np.frombuffer(binary, dtype=np.uint8)\n",
        "    image = cv2.imdecode(image, flags=cv2.IMREAD_COLOR)\n",
        "    return image\n",
        "\n",
        "# Load your pre-trained model\n",
        "model = tf.keras.models.load_model('/content/second.h5')\n",
        "\n",
        "# Start streaming video from webcam\n",
        "video_stream()\n",
        "\n",
        "# Wait a bit to let JavaScript code initialize\n",
        "time.sleep(2)\n",
        "\n",
        "# Set initial previous x, y positions and time for speed calculation\n",
        "prev_x, prev_y = None, None\n",
        "prev_time = time.time()\n",
        "\n",
        "# Amplification factor for movement sensitivity\n",
        "movement_amplification_factor = 2.0  # You can adjust this value\n",
        "\n",
        "def classify_speed(speed):\n",
        "    if speed < 10:\n",
        "        return \"very slow\"\n",
        "    elif speed < 30:\n",
        "        return \"slow\"\n",
        "    elif speed < 70:\n",
        "        return \"normal\"\n",
        "    elif speed < 150:\n",
        "        return \"fast\"\n",
        "    else:\n",
        "        return \"very fast\"\n",
        "\n",
        "# Perform a continuous loop to process the camera frames\n",
        "try:\n",
        "    while True:\n",
        "        frame = get_frame()\n",
        "        input_frame = cv2.resize(frame, (224, 224))\n",
        "        input_frame = np.expand_dims(input_frame, axis=0)\n",
        "\n",
        "        # Generate dummy input (if required by the model)\n",
        "        json_input = np.zeros((1, 5))\n",
        "\n",
        "        # Predict eye coordinates\n",
        "        prediction = model.predict([input_frame, json_input])\n",
        "\n",
        "        # Extract predicted x, y coordinates (assuming model outputs values scaled between [0, 1])\n",
        "        pred_x = prediction[0][0]\n",
        "        pred_y = prediction[0][1]\n",
        "\n",
        "        # Amplify movement for more noticeable changes\n",
        "        pred_x = int(pred_x * frame.shape[1] * movement_amplification_factor)\n",
        "        pred_y = int(pred_y * frame.shape[0] * movement_amplification_factor)\n",
        "\n",
        "        # Clamp coordinates to frame size\n",
        "        pred_x = np.clip(pred_x, 0, frame.shape[1] - 1)\n",
        "        pred_y = np.clip(pred_y, 0, frame.shape[0] - 1)\n",
        "\n",
        "        # Calculate speed of saccade (eye movement)\n",
        "        curr_time = time.time()\n",
        "        if prev_x is not None and prev_y is not None:\n",
        "            time_diff = curr_time - prev_time\n",
        "            distance = math.sqrt((pred_x - prev_x) ** 2 + (pred_y - prev_y) ** 2)\n",
        "            speed = distance / time_diff if time_diff > 0 else 0\n",
        "\n",
        "            # Update previous position and time\n",
        "            prev_time = curr_time\n",
        "            prev_x, prev_y = pred_x, pred_y\n",
        "\n",
        "            # Classify the speed of the saccade\n",
        "            speed_classification = classify_speed(speed)\n",
        "            print(f\"Saccade speed: {speed:.2f} pixels/second, classified as: {speed_classification}\")\n",
        "        else:\n",
        "            prev_x, prev_y = pred_x, pred_y\n",
        "\n",
        "        # Draw the yellow point representing eye movement\n",
        "        cv2.circle(frame, (pred_x, pred_y), 10, (0, 255, 255), -1)\n",
        "\n",
        "        # Convert frame to JPEG format and display it in the live stream\n",
        "        _, jpeg_image = cv2.imencode('.jpg', frame)\n",
        "        data_url_image = 'data:image/jpeg;base64,' + base64.b64encode(jpeg_image).decode('utf-8')\n",
        "        eval_js(f'imgElement.src=\"{data_url_image}\"; labelElement.innerText=\"Predicted: ({pred_x}, {pred_y})\";')\n",
        "\n",
        "        # Add a short delay to control frame rate\n",
        "        time.sleep(0.1)\n",
        "\n",
        "except KeyboardInterrupt:\n",
        "    pass\n"
      ],
      "metadata": {
        "id": "WGBYiZA90h_o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Pytorch tained model"
      ],
      "metadata": {
        "id": "c5rNPrb0BlO6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import display, Javascript\n",
        "from google.colab.output import eval_js\n",
        "import base64\n",
        "import cv2\n",
        "import numpy as np\n",
        "import torch\n",
        "import torchvision.transforms as transforms\n",
        "import time\n",
        "import math\n",
        "\n",
        "# Step 1: Define JavaScript code to capture video stream and snapshots\n",
        "def video_stream():\n",
        "    js = Javascript('''\n",
        "    async function videoCapture() {\n",
        "        const video = document.createElement('video');\n",
        "        document.body.appendChild(video);\n",
        "\n",
        "        const stream = await navigator.mediaDevices.getUserMedia({ video: true });\n",
        "        video.srcObject = stream;\n",
        "        await video.play();\n",
        "\n",
        "        const canvas = document.createElement('canvas');\n",
        "        canvas.width = 224;\n",
        "        canvas.height = 224;\n",
        "        document.body.appendChild(canvas);\n",
        "        const ctx = canvas.getContext('2d');\n",
        "\n",
        "        window.takeSnapshot = function() {\n",
        "            ctx.drawImage(video, 0, 0, canvas.width, canvas.height);\n",
        "            return canvas.toDataURL('image/jpeg', 0.8);\n",
        "        };\n",
        "    }\n",
        "    videoCapture();\n",
        "    ''')\n",
        "    display(js)\n",
        "\n",
        "# Step 2: Function to capture a frame from the video stream\n",
        "def get_frame():\n",
        "    data = eval_js('takeSnapshot()')  # Call the takeSnapshot function defined in JS\n",
        "    binary = base64.b64decode(data.split(',')[1])  # Decode the base64 image\n",
        "    image = np.frombuffer(binary, dtype=np.uint8)\n",
        "    return cv2.imdecode(image, cv2.IMREAD_COLOR)  # Convert image to OpenCV format\n",
        "\n",
        "# Step 3: Function to extract the eye region using OpenCV's face detection\n",
        "def extract_eye_region(frame):\n",
        "    # Convert to grayscale for face/eye detection\n",
        "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
        "    face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')\n",
        "    eye_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_eye.xml')\n",
        "\n",
        "    faces = face_cascade.detectMultiScale(gray, 1.3, 5)\n",
        "    for (x, y, w, h) in faces:\n",
        "        face_roi = gray[y:y+h, x:x+w]\n",
        "        eyes = eye_cascade.detectMultiScale(face_roi)\n",
        "        for (ex, ey, ew, eh) in eyes:\n",
        "            return frame[y+ey:y+ey+eh, x+ex:x+ex+ew]  # Return eye region\n",
        "    return frame  # Fallback to original frame if no eye detected\n",
        "\n",
        "# Step 4: Define the PyTorch model (using ResNet18 as an example)\n",
        "class GazeNetwork(torch.nn.Module):\n",
        "    def __init__(self):\n",
        "        super(GazeNetwork, self).__init__()\n",
        "        self.backbone = torch.hub.load('pytorch/vision:v0.10.0', 'resnet18', pretrained=False)\n",
        "        num_ftrs = self.backbone.fc.in_features\n",
        "        self.backbone.fc = torch.nn.Linear(num_ftrs, 2)  # Output x, y coordinates\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.backbone(x)\n",
        "\n",
        "# Step 5: Initialize the model\n",
        "model = GazeNetwork()\n",
        "\n",
        "# Step 6: Load the checkpoint\n",
        "checkpoint = torch.load('/content/epoch_24_ckpt.pth.tar', map_location=torch.device('cpu'))\n",
        "state_dict = checkpoint['model_state']  # Adjust this based on how the checkpoint was saved\n",
        "model.load_state_dict(state_dict, strict=False)\n",
        "\n",
        "# Set the model to evaluation mode\n",
        "model.eval()\n",
        "\n",
        "# Step 7: Preprocess the image for model inference\n",
        "preprocess = transforms.Compose([\n",
        "    transforms.ToPILImage(),\n",
        "    transforms.Resize((224, 224)),  # Resize the frame to 224x224, assuming the model expects this size\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  # Normalize using ImageNet stats\n",
        "])\n",
        "\n",
        "# Start the video stream\n",
        "video_stream()\n",
        "\n",
        "# Wait for JavaScript to initialize\n",
        "time.sleep(2)\n",
        "\n",
        "# Step 8: Define the function to draw a yellow point and calculate saccade speed\n",
        "prev_x, prev_y = None, None  # Store previous position\n",
        "prev_time = time.time()  # Store previous timestamp\n",
        "\n",
        "def draw_eye_movement(pred_x, pred_y):\n",
        "    global prev_x, prev_y, prev_time\n",
        "\n",
        "    # Create a blank black image to display the yellow point\n",
        "    frame = np.zeros((224, 224, 3), dtype=np.uint8)\n",
        "\n",
        "    # Normalize the predicted coordinates to cover the full range of the frame\n",
        "    pred_x = (pred_x + 1) / 2  # Map [-1, 1] to [0, 1]\n",
        "    pred_y = (pred_y + 1) / 2  # Same for Y\n",
        "\n",
        "    # Adjust scaling to make the movement more visible within the frame\n",
        "    pred_x = int(pred_x * 224)\n",
        "    pred_y = int(pred_y * 224)\n",
        "\n",
        "    # Clamp coordinates to ensure they stay within the frame boundaries\n",
        "    pred_x = np.clip(pred_x, 0, 223)\n",
        "    pred_y = np.clip(pred_y, 0, 223)\n",
        "\n",
        "    # Calculate saccade speed\n",
        "    if prev_x is not None and prev_y is not None:\n",
        "        curr_time = time.time()\n",
        "        time_diff = curr_time - prev_time\n",
        "        distance = math.sqrt((pred_x - prev_x) ** 2 + (pred_y - prev_y) ** 2)\n",
        "        speed = distance / time_diff if time_diff > 0 else 0\n",
        "\n",
        "        # Update previous time and position\n",
        "        prev_time = curr_time\n",
        "        prev_x, prev_y = pred_x, pred_y\n",
        "\n",
        "        # Classify speed\n",
        "        speed_classification = classify_speed(speed)\n",
        "        print(f\"Saccade speed: {speed:.2f} pixels/second, classified as: {speed_classification}\")\n",
        "\n",
        "    else:\n",
        "        # First frame, just set previous position\n",
        "        prev_x, prev_y = pred_x, pred_y\n",
        "\n",
        "    # Draw the yellow point representing eye movement\n",
        "    cv2.circle(frame, (pred_x, pred_y), 5, (0, 255, 255), -1)  # Yellow point\n",
        "\n",
        "    return frame\n",
        "\n",
        "# Step 9: Speed classification with adjusted thresholds\n",
        "def classify_speed(speed):\n",
        "    if speed < 5:  # Adjusted to detect very fast saccades\n",
        "        return \"very slow\"\n",
        "    elif speed < 25:\n",
        "        return \"slow\"\n",
        "    elif speed < 50:\n",
        "        return \"normal\"\n",
        "    elif speed < 100:\n",
        "        return \"high\"\n",
        "    else:\n",
        "        return \"very high\"\n",
        "\n",
        "# Step 10: Loop to capture, process, and show the frame with the yellow point\n",
        "try:\n",
        "    frame_count = 0  # Count frames to provide feedback\n",
        "    while True:\n",
        "        frame = get_frame()  # Capture a frame from the webcam\n",
        "        frame_count += 1\n",
        "\n",
        "        # Extract the eye region from the frame\n",
        "        eye_region = extract_eye_region(frame)\n",
        "\n",
        "        # Preprocess the frame for the model\n",
        "        input_image = preprocess(eye_region)\n",
        "        input_image = input_image.unsqueeze(0)  # Add batch dimension\n",
        "\n",
        "        # Run inference on the frame\n",
        "        with torch.no_grad():\n",
        "            prediction = model(input_image)\n",
        "\n",
        "        # Extract x, y coordinates from the model's output and normalize to [0, 1] range\n",
        "        pred_x = prediction[0][0].item()  # X coordinate (assuming [-1, 1] output)\n",
        "        pred_y = prediction[0][1].item()  # Y coordinate (assuming [-1, 1] output)\n",
        "\n",
        "        # Generate a smaller frame that shows only the yellow point's movement\n",
        "        yellow_point_frame = draw_eye_movement(pred_x, pred_y)\n",
        "\n",
        "        # Display the frame with the yellow point in Colab by converting to base64\n",
        "        _, img_encoded = cv2.imencode('.jpg', yellow_point_frame)\n",
        "        img_base64 = base64.b64encode(img_encoded).decode('utf-8')\n",
        "        display(Javascript(f\"\"\"\n",
        "            var img = document.getElementById('point-img');\n",
        "            if (!img) {{\n",
        "                img = new Image();\n",
        "                img.id = 'point-img';\n",
        "                document.body.appendChild(img);\n",
        "            }}\n",
        "            img.src = \"data:image/jpeg;base64,{img_base64}\";\n",
        "        \"\"\"))\n",
        "\n",
        "        # Add delay to control the frame rate\n",
        "        time.sleep(0.1)\n",
        "\n",
        "except KeyboardInterrupt:\n",
        "    pass\n"
      ],
      "metadata": {
        "id": "ys9ADM_j6BJm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "vr-TfZAMMrxY"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}